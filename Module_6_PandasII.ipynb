{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas II: Manipulating data frames\n",
    "\n",
    "Last time we learned about the ability of Python to store and manipulate tabular data with the Pandas package.\n",
    "\n",
    "Tabular data is quite common in genomic analyses.  You can imagine wanting to create and manipulate a table like the following:\n",
    "\n",
    "| Gene | expression in condition 1 | expression in condition 2 | log2 fold change | pvalue |\n",
    "|------|---------------------------|---------------------------|------------------|--------|\n",
    "|GeneA |       100                 | 200                       |        1         |   6e-5 |\n",
    "|GeneB |    50                     | 12.5                      |        -2        |  2e-6  |\n",
    "|GeneC |       40                  |   45                      |   0.17           | 0.55   |\n",
    "\n",
    "\n",
    "Ending up with a table like this often involves **merging** smaller tables together, **selecting** different columns of data from a data frame, **filtering** for rows that match specific conditions, manipulating the **shape** of the table between long and wide formats, and finally, plotting outputs.\n",
    "\n",
    "In this lesson, we will learn how to perform each of these tasks.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Selecting columns and rows\n",
    "\n",
    "For most of this session, we are going to be working with variations of the following data frame:\n",
    "\n",
    "This dataframe will be referred to as 'df' and be assigned to the variable df.  Very original, I know.\n",
    "\n",
    "| Gene | expression in condition 1 | expression in condition 2 | log2 fold change | pvalue |\n",
    "|------|---------------------------|---------------------------|------------------|--------|\n",
    "|GeneA |       100                 | 200                       |        1         |   6e-5 |\n",
    "|GeneB |    50                     | 12.5                      |        -2        |  2e-6  |\n",
    "|GeneC |       40                  |   45                      |   0.17           | 0.55   |\n",
    "|GeneD |       20                  |   20                      |   0.00           | 1.00   |\n",
    "|GeneE |        1                  |   4                       |   2.00           | 0.15   |\n",
    "|GeneF |       1000                |   800                     |   -0.32          | 0.001  |\n",
    "\n",
    "Remember that we can either create this dataframe using a dictionary:\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    d = {'Gene': ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF'],\n",
    "    'cond1exp' : [100, 50, 40, 20, 1, 1000],\n",
    "    'cond2exp' : [200, 12.5, 45, 20, 4, 800],\n",
    "    'log2FC' : [1, -2, 0.17, 0.00, 2.00, -0.32],\n",
    "    'pvalue' : [6e-5, 2e-6, 0.55, 1.00, 0.15, 0.001]}\n",
    "    \n",
    "    df = pd.DataFrame(data = d)\n",
    "    \n",
    "    \n",
    "Or we that we can load the dataframe from a file using \n",
    "\n",
    "    pd.DataFrame.from_csv\n",
    "    \n",
    "    \n",
    "Let's suppose that we wanted to make a new dataframe that consisted of only some of the columns or rows of df.  In essenece, we want to select a subset of columns and rows from df.  This is done with the **.loc** and **.iloc** methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting columns and rows with .iloc\n",
    "\n",
    "Selecting columns and rows from a dataframe by their position is easily done with the **.iloc** method.  The syntax is:\n",
    "\n",
    "    subsetdf = df.iloc[firstrowdesired:lastrowdesired, firstcolumndesired:lastcolumndesired]\n",
    "    \n",
    "For example, let's say I wanted only the first two rows of df, but every column. Remember back to when we were slicing lists (see module 2).  We could slice a list by giving the indexes of the first and last items we wanted.\n",
    "\n",
    "    somefruits = fruits[2:4]\n",
    "    \n",
    "The idea is the same here. The index values (i.e. rownames) of the desired rows are given first, followed by the names of the desired columns.  Just as when slicing lists, leaving the value on one side of the colon blank is shorthand for asking for everything from either the beginning or end.\n",
    "\n",
    "    #give me all fruits from the fourth one to the end\n",
    "    somefruits = fruits[3:]\n",
    "    #give me all fruits from the beginning until the fourth one\n",
    "    somefruits = fruits[:4]\n",
    "    \n",
    "Leaving the value blank on *both* sides of the colon gives you everything from *both* ends.  Or, in other words, the entire list (or here, all rows or all columns).\n",
    "\n",
    "> <font color = 'red'>Slightly confusing note 1: </font>  Remember, when selecting from or slicing iterables, python uses a 0-based, half-open indexing system.  This means that if you want the first 3 items, you ask for items[0:3]. You are given everything *up to but not including* the fourth item (i.e. indicies 0, 1, and 2).  iloc follows this convention.\n",
    "\n",
    "> <font color = 'red'>Slightly confusing note 2: </font> Asking for a single index using .iloc instead of a slice will give you a series instead of a dataframe.  This is essentially a list (or a numpy array), but we aren't going to talk much about this.  For selecting things within dataframes, just slice.\n",
    "\n",
    "> <font color = 'red'>Slightly confusing note 3:  </font> Pandas dataframes have things called \"indexes\" which are essentially row names.  These can be set explicitly.  However, if they are not, as in the case of df, they are simply defined as integers, beginning with 0. You can think of the indexes as just another column, but without a column name.  You can see them in the far left any time you visualize a dataframe with df.head() or any other method.\n",
    "\n",
    ".iloc operates on integer-based indexes.  This means you can ask for the third column, but not the column named 'cond2exp'.  (Conversly, with .loc, you can ask for the \n",
    "\n",
    "OK, now that we've got all that down, let's ask for just the first column of df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Defining the dataframe\n",
    "d = {'Gene': ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF'],\n",
    "    'cond1exp' : [100, 50, 40, 20, 1, 1000],\n",
    "    'cond2exp' : [200, 12.5, 45, 20, 4, 800],\n",
    "    'log2FC' : [1, -2, 0.17, 0.00, 2.00, -0.32],\n",
    "    'pvalue' : [6e-5, 2e-6, 0.55, 1.00, 0.15, 0.001]}\n",
    "    \n",
    "df = pd.DataFrame(data = d)\n",
    "\n",
    "#Take a quick look at it.\n",
    "print(df.head(), '\\n\\n\\n\\n') #print some blank lines so we can tell things apart\n",
    "\n",
    "#Get the first column (and every row) of df\n",
    "subsetdf = df.iloc[:, 0:1] #give me all rows and just the first column\n",
    "\n",
    "print(subsetdf.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, great.  What about just the third row and all the columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsetdf = df.iloc[2:3, :]\n",
    "print(subsetdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now the second through fourth columns and the first through fifth rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsetdf = df.iloc[0:5, 1:4]\n",
    "print(subsetdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting rows and columns with .loc\n",
    "\n",
    "**.loc** is essentially the same as **.iloc**, except that you can now select by column names or row names.  The same ideas with slicing and ranges apply.  If row names (indexes) have not been explicitly set for a dataframe, they are defined as integers beginning with 0, as discussed above.\n",
    "\n",
    "> <font color = 'red'>Confusing note number 4:</font> although most intervals are half-open, **for .loc, they are closed**.  df.loc[:, 'Gene':'log2FC'] will return all rows and all columns between Gene and log2FC **including log2FC**.\n",
    "\n",
    "The desired rows and/or columns are indicated by putting them in a list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select the expression values in condition1\n",
    "\n",
    "subsetdf = df.loc[:, ['cond1exp']]\n",
    "print(subsetdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Asking for the column name as a string rather than within a list returns a numpy array instead of a one-column dataframe\n",
    "#This allows us to easily turn the values into a list\n",
    "\n",
    "subsetdf = df.loc[:, 'cond1exp'] #notice cond1exp is a string, not contained within a list\n",
    "print(subsetdf,  '\\n\\n\\n\\n')\n",
    "subsetdf = subsetdf.tolist() #turn into list\n",
    "print(subsetdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select only the 3rd row\n",
    "subsetdf = df.loc[[3], :]\n",
    "print(subsetdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select the second through fifth columns\n",
    "subsetdf = df.loc[:, ['cond1exp', 'cond2exp', 'log2FC', 'pvalue']]\n",
    "print(subsetdf.head(), '\\n\\n\\n\\n')\n",
    "\n",
    "#This is equivalent to using a range and slicing\n",
    "#When slicing, the ids do not go in a list\n",
    "subsetdf = df.loc[:, 'cond1exp':'pvalue']\n",
    "print(subsetdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting rows based on conditions\n",
    "\n",
    "We've learned how to select data (columns and/or rows) based on their positions within a dataframe or the names of the columns/rows.  Now we will discuss how to select rows based on whether or not their values meet certain conditions.\n",
    "\n",
    "For example, say we wanted to select all genes whose expression level in condition 1 was at least 20.\n",
    "\n",
    "This can be done using the **.query** method.\n",
    "\n",
    "Query accepts a string as an argument.  Pandas parses this string to evaluate whether or not rows meet the conditions within the string.  Standard boolean operators like **and** and **or** can be used within the string as **&** and **|**, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Defining the dataframe\n",
    "d = {'Gene': ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF'],\n",
    "    'cond1exp' : [100, 50, 40, 20, 1, 1000],\n",
    "    'cond2exp' : [200, 12.5, 45, 20, 4, 800],\n",
    "    'log2FC' : [1, -2, 0.17, 0.00, 2.00, -0.32],\n",
    "    'pvalue' : [6e-5, 2e-6, 0.55, 1.00, 0.15, 0.001]}\n",
    "    \n",
    "df = pd.DataFrame(data = d)\n",
    "\n",
    "#Select rows whose expression in condition 1 is 100\n",
    "newdf = df.query('cond1exp == 100')\n",
    "\n",
    "print(newdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select rows whose expression in condition 1 is at least 20\n",
    "newdf = df.query('cond1exp >= 20')\n",
    "\n",
    "print(newdf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Select rows whose expression in condition 1 is at least 20 and the pvalue is less than 0.05\n",
    "newdf = df.query('cond1exp >= 20 & pvalue < 0.05')\n",
    "\n",
    "print(newdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new columns\n",
    "\n",
    "Suppose we wanted to add a new column of data.\n",
    "\n",
    "This can be done with the **.assign** method.\n",
    "\n",
    "    newdf = df.assign(colname = <list of column values>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the dataframe\n",
    "d = {'Gene': ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF'],\n",
    "    'cond1exp' : [100, 50, 40, 20, 1, 1000],\n",
    "    'cond2exp' : [200, 12.5, 45, 20, 4, 800],\n",
    "    'log2FC' : [1, -2, 0.17, 0.00, 2.00, -0.32],\n",
    "    'pvalue' : [6e-5, 2e-6, 0.55, 1.00, 0.15, 0.001]}\n",
    "    \n",
    "df = pd.DataFrame(data = d)\n",
    "\n",
    "#Add the expression in a third condition\n",
    "\n",
    "cond3expvalues = [20, 50, 10, 1000, 10, 2] #these are IN ORDER\n",
    "\n",
    "newdf = df.assign(cond3exp = cond3expvalues)\n",
    "print(newdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we wanted to add a new column, but it's values were based on the values of other columns.  Let's say we wanted to add a new column called 'otherlog2FC'.  This is the same as the log2FC we currently have, but instead of log2(cond2exp / cond1exp), we want it to be log2(cond1exp/cond2exp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#A new log2FC column\n",
    "\n",
    "#First we need to get a list of the values that will make up our new column\n",
    "cond1exp = df['cond1exp'] #this is a pandas series of the values in the cond1exp column, essentially a one-column dataframe\n",
    "cond2exp = df['cond2exp']\n",
    "newlog2FCvalues = np.log2(cond1exp / cond2exp)\n",
    "print(newlog2FCvalues)\n",
    "\n",
    "#Now add these values as a new column\n",
    "newdf = df.assign(newlog2FC = newlog2FCvalues)\n",
    "print(newdf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new rows\n",
    "\n",
    "Adding new rows is essentially the same as appending one data frame (even if it's only one row long) to the end of another data frame.  Not surprisingly, then, this is done with the **.append** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We now have data for GeneG\n",
    "\n",
    "d = {'Gene': ['GeneG'], 'cond1exp': [10], 'cond2exp': [40], 'log2FC' : [2], 'pvalue' : [4e-5]}\n",
    "df2 = pd.DataFrame(data = d)\n",
    "print(df2.head(), '\\n\\n\\n\\n')\n",
    "\n",
    "#Add df2 to the bottom of df\n",
    "newdf = df.append(df2)\n",
    "print(newdf, '\\n\\n\\n\\n')\n",
    "\n",
    "#Oh no, df2 kept it's original index (0)! We want it to have a new index based on its position in the new df\n",
    "newdf = df.append(df2, ignore_index = True)\n",
    "print(newdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating through rows\n",
    "\n",
    "Perhaps you want to iterate through rows, performing some kind of action on the data in each row.  To do this, we can use the **.iterrows()** method.  This returns a generator which yields both the index and the row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the dataframe\n",
    "d = {'Gene': ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF'],\n",
    "    'cond1exp' : [100, 50, 40, 20, 1, 1000],\n",
    "    'cond2exp' : [200, 12.5, 45, 20, 4, 800],\n",
    "    'log2FC' : [1, -2, 0.17, 0.00, 2.00, -0.32],\n",
    "    'pvalue' : [6e-5, 2e-6, 0.55, 1.00, 0.15, 0.001]}\n",
    "    \n",
    "df = pd.DataFrame(data = d)\n",
    "print(df)\n",
    "\n",
    "#For each row, print the cond1exp value and cond2exp value\n",
    "for index, row in df.iterrows():\n",
    "    print(row['cond1exp'], row['cond2exp'])\n",
    "\n",
    "#You can use this to add columns if you like\n",
    "#Think of index as the rowname\n",
    "for index, row in df.iterrows():\n",
    "    totalexpression = row['cond1exp'] + row['cond2exp']\n",
    "    #Add this as a new column named 'totexp' using .loc.\n",
    "    if totalexpression < 50:\n",
    "        df.loc[index, 'totexp'] = 'low'\n",
    "    elif totalexpression >= 50 and totalexpression < 100:\n",
    "        df.loc[index, 'totexp'] = 'medium'\n",
    "    elif totalexpression > 100:\n",
    "        df.loc[index, 'totexp'] = 'high'\n",
    "    \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging dataframes together\n",
    "\n",
    "Often it is useful to combine datasets together.  For example, let's say someone else did a similar experiment.  You might want to take their data and compare it to yours to see how similar it is.  You can do this by merging your tables together.  This must be done on a key.  For example, say your data is below:\n",
    "\n",
    "| Gene | expression in condition 1 | expression in condition 2 | log2 fold change | pvalue |\n",
    "|------|---------------------------|---------------------------|------------------|--------|\n",
    "|GeneA |       100                 | 200                       |        1         |   6e-5 |\n",
    "|GeneB |    50                     | 12.5                      |        -2        |  2e-6  |\n",
    "|GeneC |       40                  |   45                      |   0.17           | 0.55   |\n",
    "|GeneD |       20                  |   20                      |   0.00           | 1.00   |\n",
    "|GeneE |        1                  |   4                       |   2.00           | 0.15   |\n",
    "|GeneF |       1000                |   800                     |   -0.32          | 0.001  |\n",
    "\n",
    "and your friend's data looks like this:\n",
    "\n",
    "| Gene | expression in condition 1 | expression in condition 2 | log2 fold change | pvalue |\n",
    "|------|---------------------------|---------------------------|------------------|--------|\n",
    "|GeneA |       120                 | 210                       |        0.81         |   2e-3 |\n",
    "|GeneB |    50                     | 25                      |        -1        |  1e-4  |\n",
    "|GeneC |       30                  |   25                      |   0.26           | 0.43   |\n",
    "|GeneD |       20                  |   30                      |   -0.58           | 0.25   |\n",
    "|GeneE |        2                  |   5                       |   1.32           | 0.31   |\n",
    "|GeneF |       950                |   600                     |   -0.66          | 0.005  |\n",
    "\n",
    "It might be useful to combine these into one table so that you can look at all the values for a particular gene at once.\n",
    "\n",
    "This can be handled with the **.merge** function.  Merge requires a 'key' (or keys) to merge on.  These are the items that you want to use a reference points that join the two dataframes.  In this case, a key that makes sense is 'Gene'.  We want each row to be one gene followed by all the data associated with it.\n",
    "\n",
    "    mergeddf = pd.merge(leftdf, rightdf, how = <string>, on = <string or list>)\n",
    "    \n",
    "The 'how' argument wants a string telling how the merge should be done.  The options are (left, right, outer, inner).  More on that in a bit.  The on argument is the name of the column you intend to use as the key for merging.  In this case, we would like it to be 'Gene'.  If you would like to join on two or more keys simultaneously, provide multiple strings in a list.\n",
    "\n",
    "### 'How' to merge\n",
    "\n",
    "As stated above, the four ways you can join your tables together are left, right, outer, and inner.  These have to do with dealing with keys that are present in one table but not the other.  Joining 'left' will return a merged dataframe that only has keys (rows) that were present in the left table.  Those that were present in the right table *but not in the left table* are not included here.  Joining 'right' works similarly but only includes keys that were present in the right table.  Joining 'inner' only includes keys that are present in both (the intersection).  Joining 'outer' includes keys that were present in either (the union).  \n",
    "![alt text](https://shanelynnwebsite-mid9n9g1q9y8tt.netdna-ssl.com/wp-content/uploads/2017/03/join-types-merge-names.jpg \"Title\")\n",
    "\n",
    "In this first example of merging our two gene expression tables together, how we join them does not matter since every key in one table is also present in the other.  However, you can imagine scenarios (essentially every time you would want to do this with real data) where this would not be the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Merge your two dataframes using the gene names as the key.\n",
    "\n",
    "d = {'Gene': ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF'],\n",
    "    'cond1exp' : [100, 50, 40, 20, 1, 1000],\n",
    "    'cond2exp' : [200, 12.5, 45, 20, 4, 800],\n",
    "    'log2FC' : [1, -2, 0.17, 0.00, 2.00, -0.32],\n",
    "    'pvalue' : [6e-5, 2e-6, 0.55, 1.00, 0.15, 0.001]}\n",
    "\n",
    "df = pd.DataFrame(data = d) #your data\n",
    "\n",
    "d2 = {'Gene': ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF'],\n",
    "    'cond1exp' : [120, 50, 30, 20, 2, 950],\n",
    "    'cond2exp' : [210, 25, 25, 30, 5, 600],\n",
    "    'log2FC' : [0.81, -1, 0.26, -0.58, 1.32, 0.66],\n",
    "    'pvalue' : [2e-3, 1e-4, 0.43, 0.25, 0.31, 0.005]}\n",
    "\n",
    "df2 = pd.DataFrame(data = d2) #your friend's data\n",
    "\n",
    "mergeddf = pd.merge(df, df2, how = 'left', on = 'Gene')\n",
    "\n",
    "print(mergeddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, but if possible, I'd like to know which value came from my experiment and which came from my friends.  Right now, these are indicated by the _x and _y suffixes, but we can give more meaningful suffixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mergeddf = pd.merge(df, df2, how = 'left', on = 'Gene', suffixes = ['_mine', '_myfriends'])\n",
    "\n",
    "print(mergeddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping data\n",
    "\n",
    "In general, tabular data can either be in 'wide' or 'long' format.  These are illustrated below:\n",
    "\n",
    "This is a 'wide' table.  Each data variable has its own column.  It's perhaps more human readable, but addition of a new variable will require the creation of a new column.  \n",
    "\n",
    "| Name | Age | Weight |\n",
    "|------|---------------------------|---------------------------|\n",
    "|Bob |       32                 | 128                       |\n",
    "|Alice |    24                     | 86                      |\n",
    "|Steve |       64                  |   95                     |\n",
    "\n",
    "\n",
    "Most of the time, when you have multiple variables related to a single item, it can be easier to have the data in 'long' format.\n",
    "\n",
    "| Name | Variable | Value |\n",
    "|------|---------------------------|---------------------------|\n",
    "|Bob |       Age                 | 32                       | \n",
    "|Bob |    Weight                 | 128                      |\n",
    "|Alice |       Age                  |   24                      | \n",
    "|Alice |       Weight                 |   86                      | \n",
    "|Steve |        Age              |   64                       | \n",
    "|Steve |       Weight                |   95                     | \n",
    "\n",
    "This is the same data, just in a different form.  Addition of a new variable (say...gender) does not require the creation of a new column. Most plotting functions will prefer a table in long format over wide format.\n",
    "\n",
    "The **melt()** function easily transforms wide data to long data.\n",
    "\n",
    "    melteddf = pd.melt(df, id_vars, value_vars)\n",
    "    \n",
    "id_vars is a list of the column (or columns) you wish to use as the identifier.  In the example above, this would be ['Name'].  value_vars is a list of the column (or columns) you wish to retain values of.  In the example above, this would be ['Age', 'Weight'].  By default, this is all columns that are *not* in the id_vars list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Melt our gene expression df, keeping the data associated with each variable\n",
    "\n",
    "d = {'Gene': ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF'],\n",
    "    'cond1exp' : [100, 50, 40, 20, 1, 1000],\n",
    "    'cond2exp' : [200, 12.5, 45, 20, 4, 800],\n",
    "    'log2FC' : [1, -2, 0.17, 0.00, 2.00, -0.32],\n",
    "    'pvalue' : [6e-5, 2e-6, 0.55, 1.00, 0.15, 0.001]}\n",
    "\n",
    "df = pd.DataFrame(data = d)\n",
    "print(df, '\\n\\n\\n\\n')\n",
    "\n",
    "melteddf = pd.melt(df, id_vars = ['Gene'], value_vars = ['cond1exp', 'cond2exp', 'log2FC', 'pvalue'])\n",
    "print(melteddf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "\n",
    "OK so we've done our analysis, we've filtered and merged our dataframe so it's just like we want.  Now it's time for the payoff: plotting.  There are a few plotting packages written for Python, but we will be using **seaborn**.  It is still being actively developed, is reasonably mature, and is definitely the most beautiful.  You can read more about everything seaborn can do at https://seaborn.pydata.org/.\n",
    "\n",
    "For now, we are just going to get our feet wet by making some simple plots.  The seaborn tutorial (https://seaborn.pydata.org/tutorial.html) is quite good, and to be honest, better than what we could cover ourselves here.  We will focus here on making plots of our toy data to get you initiated.\n",
    "\n",
    "Seaborn plots can take pandas dataframes as inputs in the 'data' argument.  Let's try to make a barplot of our gene expression values.\n",
    "\n",
    "> Note: this is not a particularly good analysis.  Don't do it in your next paper.  This is just for fun.\n",
    "\n",
    "As a reminder, here's our data again:\n",
    "\n",
    "| Gene | expression in condition 1 | expression in condition 2 | log2 fold change | pvalue |\n",
    "|------|---------------------------|---------------------------|------------------|--------|\n",
    "|GeneA |       100                 | 200                       |        1         |   6e-5 |\n",
    "|GeneB |    50                     | 12.5                      |        -2        |  2e-6  |\n",
    "|GeneC |       40                  |   45                      |   0.17           | 0.55   |\n",
    "|GeneD |       20                  |   20                      |   0.00           | 1.00   |\n",
    "|GeneE |        1                  |   4                       |   2.00           | 0.15   |\n",
    "|GeneF |       1000                |   800                     |   -0.32          | 0.001  |\n",
    "\n",
    "> Note 2: After we are done with the code to make the plot, we need one more line, plt.show(), to get the plot to actually show.  Many aesthetic details (axis labels, grids, etc.) can be added to the plot after making it, but before showing it.  Seaborn is based on an earlier plotting package called matplotlib.  Aesthetic changes to seaborn plots can therefore be made with matplotlib commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Our data\n",
    "d = {'Gene': ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF'],\n",
    "    'cond1exp' : [100, 50, 40, 20, 1, 1000],\n",
    "    'cond2exp' : [200, 12.5, 45, 20, 4, 800],\n",
    "    'log2FC' : [1, -2, 0.17, 0.00, 2.00, -0.32],\n",
    "    'pvalue' : [6e-5, 2e-6, 0.55, 1.00, 0.15, 0.001]}\n",
    "\n",
    "df = pd.DataFrame(data = d)\n",
    "\n",
    "#We want a 'barplot'\n",
    "\n",
    "myplot = sns.barplot(data = df, x = 'Gene', y = 'cond1exp')\n",
    "plt.show()\n",
    "\n",
    "#Nice, but I like red bars.\n",
    "myplot = sns.barplot(data = df, x = 'Gene', y = 'cond1exp', color = 'red')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad.  But let's say I want to see their expression in condition1 *and* condition2 in the same plot.  You'll notice that in the sns.barplot() call, there is only *x* and *y* arguments that determine what goes on each axis.  So, with the table as it is now, how can we show **both** conditions?  The short answer is you can't.  However, if we reorganized the table a bit, we could.  In fact, if we **melted** this table to it from wide into long data, we could.  See why below.\n",
    "\n",
    "This is our original data:\n",
    "\n",
    "| Gene | cond1exp | cond2exp | log2FC | pvalue |\n",
    "|------|---------------------------|---------------------------|------------------|--------|\n",
    "|GeneA |       100                 | 200                       |        1         |   6e-5 |\n",
    "|GeneB |    50                     | 12.5                      |        -2        |  2e-6  |\n",
    "|GeneC |       40                  |   45                      |   0.17           | 0.55   |\n",
    "|GeneD |       20                  |   20                      |   0.00           | 1.00   |\n",
    "|GeneE |        1                  |   4                       |   2.00           | 0.15   |\n",
    "|GeneF |       1000                |   800                     |   -0.32          | 0.001  |\n",
    "\n",
    "It's obvious that the x axis should be 'Gene', but what should the y axis be?  Condition1 or condition2? Remember that we'd like to show both on the same plot.  If we melted the table to make it long we would come up with this:\n",
    "\n",
    "| Gene | variable | value |\n",
    "|------|---------------------------|---------------------------|\n",
    "|GeneA |       cond1exp                 | 100                       |\n",
    "|GeneB |    cond1exp                     | 50                      |\n",
    "|GeneC |       cond1exp                  |   40                      |\n",
    "|GeneD |       cond1exp                  |   20                      |\n",
    "|GeneE |        cond1exp                  |   1                       |\n",
    "|GeneF |       cond1exp                |   1000                     |\n",
    "|GeneA |       cond2exp                |   200                     |\n",
    "|GeneB |       cond2exp                |   12.5                     |\n",
    "|GeneC |       cond2exp                |   45                     |\n",
    "|GeneD |       cond2exp                |   20                     |\n",
    "|GeneE |       cond2exp                |   4                     |\n",
    "|GeneF |       cond2exp                |   800                     |\n",
    "\n",
    "Now, the x axis can still be 'Gene' and the y axis can be 'value'.  All we need to do then is come up with a way of differentiating based on the value in 'variable'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'Gene': ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF'],\n",
    "    'cond1exp' : [100, 50, 40, 20, 1, 1000],\n",
    "    'cond2exp' : [200, 12.5, 45, 20, 4, 800],\n",
    "    'log2FC' : [1, -2, 0.17, 0.00, 2.00, -0.32],\n",
    "    'pvalue' : [6e-5, 2e-6, 0.55, 1.00, 0.15, 0.001]}\n",
    "\n",
    "df = pd.DataFrame(data = d)\n",
    "\n",
    "#Melt df\n",
    "\n",
    "melteddf = pd.melt(df, id_vars = ['Gene'], value_vars = ['cond1exp', 'cond2exp'])\n",
    "print(melteddf)\n",
    "\n",
    "myplot = sns.barplot(data = melteddf, x = 'Gene', y = 'value', hue = 'variable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad.  Maybe instead we want to make a boxplot of all the genes within a sample.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'Gene': ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF'],\n",
    "    'cond1exp' : [100, 50, 40, 20, 1, 1000],\n",
    "    'cond2exp' : [200, 12.5, 45, 20, 4, 800],\n",
    "    'log2FC' : [1, -2, 0.17, 0.00, 2.00, -0.32],\n",
    "    'pvalue' : [6e-5, 2e-6, 0.55, 1.00, 0.15, 0.001]}\n",
    "\n",
    "df = pd.DataFrame(data = d)\n",
    "\n",
    "#Melt df\n",
    "\n",
    "melteddf = pd.melt(df, id_vars = ['Gene'], value_vars = ['cond1exp', 'cond2exp'])\n",
    "\n",
    "sns.boxplot(data = melteddf, x = 'variable', y = 'value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now what about a scatter for comparing values from the two conditions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "d = {'Gene': ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF'],\n",
    "    'cond1exp' : [100, 50, 40, 20, 1, 1000],\n",
    "    'cond2exp' : [200, 12.5, 45, 20, 4, 800],\n",
    "    'log2FC' : [1, -2, 0.17, 0.00, 2.00, -0.32],\n",
    "    'pvalue' : [6e-5, 2e-6, 0.55, 1.00, 0.15, 0.001]}\n",
    "\n",
    "df = pd.DataFrame(data = d)\n",
    "\n",
    "#No need to melt this time.  x = cond1, y = cond2\n",
    "\n",
    "#plot their relation with a relplot\n",
    "\n",
    "sns.relplot(data = df, x = 'cond1exp', y = 'cond2exp')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A message about overplotting\n",
    "\n",
    "Scatterplots are common in genomic analysis.  Often, you are dealing with tens or hundreds of thousands of observations, and scatterplots are quick and easy ways to look for relationships between variables.  However, they should be used with some caution.  One common pitfall of this type of plot is overplotting.  This occurs when the density of points on a plot obscures the true nature of the data.\n",
    "\n",
    "Consider the scatterplot below.  It seems obvious that there is a correlation between x and y (there is).  It also looks like the density of the data in the x direction is more or less constant between x = 0 and x = 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](scatter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this is misleading.  It would be nice to somehow represent the density of the dots at each location.  This would give us information about how much of the data lies at each point.  One way this can be done is by using a hexbin plot.  We aren't going to get into how to make them right now, but you can read their documentation [here](https://seaborn.pydata.org/examples/hexbin_marginals.html).\n",
    "\n",
    "Using this approach, we can see that, in actuality, the vast majority of the data is contained between x = 0 and x = 2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hexbin](hexbin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercises\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "Often, it's not a good idea to work on raw expression values.  Work on tranformed (for example, log-transformed) can sometimes be more informative (if not more mathematically correct).  The scatter plot we produced to compare gene expression values in conditions 1 and 2 is a good example.  It would be better to instead look at the correlation between logged (let's say log10) values.  Produce a scatter plot comparing the logged expression values.\n",
    "\n",
    "Hints:\n",
    "- Add two new columns to the dataframe that contain the logged values in conditions 1 and 2, respectively.\n",
    "- You may (will) need the numpy.log10() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'Gene': ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF'],\n",
    "    'cond1exp' : [100, 50, 40, 20, 1, 1000],\n",
    "    'cond2exp' : [200, 12.5, 45, 20, 4, 800],\n",
    "    'log2FC' : [1, -2, 0.17, 0.00, 2.00, -0.32],\n",
    "    'pvalue' : [6e-5, 2e-6, 0.55, 1.00, 0.15, 0.001]}\n",
    "\n",
    "df = pd.DataFrame(data = d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Your friend has done a very similar experiment, looking at the expression of the same genes in the same conditions.  You might like to know how well your two datasets agree with each other.  One way to do this would be to correlate the log2 fold changes that you observed with the log2 fold changes that your friend observed.  Write code to obtain the Spearman correlation coefficient for your two log2 fold changes.  \n",
    "\n",
    "Now, is the correlation stronger or weaker than what you would expect by chance?  One way to find this out would be to randomize your data and get correlations for the random data.  If you did this many times (say, 500 times) you could see where your real correlation coefficient fell within the distribution of control random correlations.  Write code to do this, finishing with a z-score comparison of the real correlation coefficient and the random values.\n",
    "\n",
    "> Note: A z-score compares a single value (like your real corrleation) to a distribution of other values (like all the random correlations).  Lets define your single value as x and the distribution of the other values as y.  The z-score is defined as (mean(y) - x) / (standarddeviation(y))\n",
    "\n",
    "Hints:\n",
    "- You can get the spearman correlation between two lists of numbers using scipy.stats.spearmanr().  This function assumes that the order of the values in each list is paired. Think of it like a scatter plot.  For each point, the value in the first list is the x coordinate and the value in the second list is its y coordinate.\n",
    "- You can randomly shuffle the order of values within a list using shuffle(), which is in the random package.  Importantly, this shuffles the list **in place**.\n",
    "- You can get the mean of a distribution (also known as a list) with mean(), which is in the numpy package.\n",
    "- You can get the standard deviation of a distribution (also known as a list) with std(), which is in the numpy package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from numpy import std\n",
    "from random import shuffle\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "#Your data\n",
    "d = {'Gene': ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF'],\n",
    "    'cond1exp' : [100, 50, 40, 20, 1, 1000],\n",
    "    'cond2exp' : [200, 12.5, 45, 20, 4, 800],\n",
    "    'log2FC' : [1, -2, 0.17, 0.00, 2.00, -0.32],\n",
    "    'pvalue' : [6e-5, 2e-6, 0.55, 1.00, 0.15, 0.001]}\n",
    "\n",
    "yourdata = pd.DataFrame(data = d)\n",
    "\n",
    "#Your friend's data\n",
    "d2 = {'Gene': ['GeneA', 'GeneB', 'GeneC', 'GeneD', 'GeneE', 'GeneF'],\n",
    "    'cond1exp' : [120, 50, 30, 20, 2, 950],\n",
    "    'cond2exp' : [210, 25, 25, 30, 5, 600],\n",
    "    'log2FC' : [0.81, -1, 0.26, -0.58, 1.32, 0.66],\n",
    "    'pvalue' : [2e-3, 1e-4, 0.43, 0.25, 0.31, 0.005]}\n",
    "\n",
    "friendsdata = pd.DataFrame(data = d2)\n",
    "\n",
    "\n",
    "mylog2 = yourdata.loc[:, 'log2FC'].values\n",
    "friendlog2 = friendsdata.loc[:, 'log2FC'].values\n",
    "\n",
    "realcorr = spearmanr(mylog2, friendlog2)[0]\n",
    "\n",
    "randomcors = []\n",
    "for i in range(500):\n",
    "    shuffle(mylog2) #shuffles in place\n",
    "    shuffle(friendlog2) #shuffles in place\n",
    "    randomcor = spearmanr(mylog2, friendlog2)[0]\n",
    "    randomcors.append(randomcor)\n",
    "    \n",
    "randommean = mean(randomcors)\n",
    "randomsd = std(randomcors)\n",
    "\n",
    "z = (realcorr - randommean) / randomsd\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
